---
title: "Untitled"
author: "Kyle Hommes"
date: "5/11/2018"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
Packages Needed:
```{r,echo=TRUE,warning=FALSE,message=FALSE,,error=FALSE}
library(plyr)
library(dplyr)
library(ggplot2)
library(wordcloud)
library(wordcloud2)
library(twitteR)
library(webshot)
library(htmlwidgets)
library(data.table)
library(tm)
library(tidyr)
library(RColorBrewer)
library(tidytext)
library(stringr)
library(reshape2)
library(topicmodels)
library(RTextTools)
library(ggmap)
library(topicmodels)
library(lsa)
library(SnowballC)
library(scatterplot3d)
library(LSAfun)
library(maps)
library(cluster)
library(kmed)
library(NbClust)
library(factoextra)
```
```{r wordcloud,include=FALSE}
# Code to include wordcloud in .pdf knit
my_graph=wordcloud2(demoFreq, size=1.5)
saveWidget(my_graph,"tmp.html",selfcontained = F)
webshot("tmp.html","wc1.png", delay =15, vwidth = 2000, vheight=2000)
```
Setup Twitter API:
```{r}
setup_twitter_oauth(consumer_key = consumer.key, 
  consumer_secret = consumer.secret, access_token = access.token,
  access_secret = access.secret)
```



Pull Tweets for the following hashtags:

Code for extracting the tweets is in Twitter Pulls.R. The hashtags pulled were:

#mentalhealthawareness
#anxiety
#depression
#ptsd
#suicide
```{r}
```
Exploratory Data Analysis:
```{r,error=FALSE,warning=FALSE,message=FALSE}
wordcloud2(mhafreq.df)
wordcloud2(afreq.df)
wordcloud2(pfreq.df)
wordcloud2(sfreq.df)
wordcloud2(dfreq.df)
wordcloud2(totfreq.df)
wordcloud(mhafreq.df$word,mhafreq.df$freq,max.words=100,colors=brewer.pal(8,"Dark2"),scale=c(3,0.5),random.order=F)
ggplot(totaltweet) + geom_bar(aes(x = isRetweet, fill = isRetweet),
  stat = 'count') + geom_text(stat = 'count', 
  aes(x = isRetweet,label=..count..), vjust = 3) + 
  labs(x = "Retweeted Tweets", y = "Count", 
  fill = "Retweeted", 
  title = "Retweeted and Non-Retweeted\nTweets for All Hashtags")
ggplot(totaltweet) + geom_bar(aes(x = isRetweet, fill = hashtag)) + 
  facet_wrap(~hashtag) + geom_text(stat = 'count',
  aes(x = isRetweet, label=..count..), vjust = 1) +
  labs(x = "Retweeted Tweets", y = "Count", fill = "Hashtag",
  title = "Retweeted and Non-Retweeted\nTweets per Hashtag")
worldMap <- map_data("world")
loc_plot <- ggplot(worldMap)
loc_plot + geom_path(aes(x = long, y = lat, group = group)) +
  geom_point(data = location,aes(x=lon,y=lat),color="RED",
  size=.25) + labs(x = "Longitude", y = "Latitude", 
  title = "Map of Location for #mentalhealthawarness Tweets")
totaltweet %>%
  ungroup() %>% filter(!str_detect(text, "^RT")) %>%
  mutate(text = str_replace_all(text, replace_reg, "")) %>%
  unnest_tokens(word, text, token = "regex", 
  pattern = unnest_reg) %>%
  filter(!word %in% stop_words$word, 
  str_detect(word, "[a-z]")) %>%
  dplyr::count(word) %>%
  with(wordcloud(word, n, max.words = 100,
  colors=brewer.pal(8,"Dark2"), 
  scale=c(1.5,0.3), random.order=F))
hashtag_words <- totaltweet %>% 
  filter(!str_detect(text, "^RT")) %>%
  mutate(text = str_replace_all(text, replace_reg, "")) %>%
  unnest_tokens(word, text, token = "regex", 
  pattern = unnest_reg) %>%
  filter(!word %in% stop_words$word,
  str_detect(word, "[a-z]")) %>% 
  dplyr::count(hashtag, word, sort = TRUE) %>% ungroup()
total_words <- hashtag_words %>% group_by(hashtag) %>%
  dplyr::summarize(total = sum(n))
hashtag_words <- left_join(hashtag_words,total_words)
ggplot(hashtag_words, aes(n/total, fill = hashtag)) +
  geom_histogram(show.legend = FALSE) +
  xlim(NA, 0.0009) +
  facet_wrap(~hashtag, ncol = 2, scales = "free_y") +
  labs(x = "Percentage of Total Word Usage", y = "Frequency", 
  title = "Word Frequency Distribution per Hashtag")
hashtag_tfidf <- hashtag_words %>% bind_tf_idf(word, hashtag, n)
hashtag_tfidf <- hashtag_tfidf %>% dplyr::select(-total) %>%
  arrange(desc(tf_idf))
hashtag_tfidf %>%
  arrange(desc(tf_idf)) %>%
  mutate(word = factor(word, levels = rev(unique(word)))) %>% 
  group_by(hashtag) %>% 
  top_n(15) %>% 
  ungroup %>%
  ggplot(aes(word, tf_idf, fill = hashtag)) +
  geom_col(show.legend = FALSE) +
  labs(x = NULL, y = "tf-idf") +
  facet_wrap(~hashtag, ncol = 2, scales = "free") +
  coord_flip() + labs(y = "Frequency of Use", x = "Word",
  title = "Most Used Words by Hashtag")
freq_by_rank <- hashtag_words %>% 
  group_by(hashtag) %>% 
  mutate(rank = row_number(), `term frequency` = n/total)
freq_by_rank %>% 
  ggplot(aes(rank, `term frequency`, color = hashtag)) + 
  geom_line(size = 1.1, alpha = 0.8, show.legend = TRUE) + 
  scale_x_log10() + scale_y_log10() + labs(x = "Rank", 
  y = "Term Frequency", 
  title = "Zipf's Plot of each Hashtag", color = "Hashtag")
```
Sentiment Analysis:
```{r,error=FALSE,message=FALSE,warning=FALSE}
# Sentiment analysis code adapted from:
# https://www.tidytextmining.com/
tweet_sentiment <- tidytweet %>%
  inner_join(get_sentiments("bing")) %>%
  dplyr::count(hashtag, index = linenumber %/% 50,sentiment) %>%
  spread(sentiment, n, fill = 0) %>%
  mutate(sentiment = positive - negative)
tweet_sentiment_nrc <- tidytweet %>%
  inner_join(get_sentiments("nrc")) %>%
  group_by(index = linenumber %/% 50, hashtag, sentiment) %>%
  summarize(n = n())
ggplot(tweet_sentiment_nrc) + 
  geom_bar(aes(x = index, y = n, fill = hashtag), 
  stat = "identity") + facet_wrap(~sentiment, ncol = 5)
ggplot(tweet_sentiment,aes(index, sentiment,fill = hashtag)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~hashtag, ncol = 2, scales = "free_x")
bing_word_counts <- tidytweet %>%
  inner_join(get_sentiments("bing")) %>%
  dplyr::count(word, sentiment, sort = TRUE) %>%
  ungroup()
bing_word_hashtag_counts <- tidytweet %>%
  inner_join(get_sentiments("bing")) %>%
  dplyr::count(word, hashtag, sentiment, sort = TRUE) %>%
  ungroup()
bing_word_counts %>%
  group_by(sentiment) %>%
  top_n(15) %>%
  ungroup() %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n, fill = sentiment)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~sentiment, scales = "free_y") +
  labs(y = "Contribution to sentiment",x = NULL) +
  coord_flip()
bing_word_hashtag_counts %>%
  group_by(hashtag,sentiment) %>%
  top_n(5) %>%
  ungroup() %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n, fill = sentiment)) +
  geom_col(show.legend = TRUE) +
  facet_wrap(~hashtag, scales = "free_y") +
  labs(y = "Contribution to sentiment",x = NULL) +
  coord_flip()
bing_word_counts_nrc <- tidytweet %>%
  inner_join(get_sentiments("nrc")) %>%
  dplyr::count(word, sentiment, sort = TRUE) %>%
  ungroup()
bing_word_counts_nrc %>%
  group_by(sentiment) %>%
  top_n(10) %>%
  ungroup() %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n, fill = sentiment)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~sentiment, scales = "free_y") +
  labs(y = "Contribution to sentiment",x = NULL) +
  coord_flip()
bing_word_hashtag_counts_nrc <- tidytweet %>%
  inner_join(get_sentiments("nrc")) %>%
  dplyr::count(word, hashtag, sentiment, sort = TRUE) %>%
  ungroup()
bing_word_hashtag_counts_nrc %>%
  group_by(hashtag,sentiment) %>%
  top_n(2) %>%
  ungroup() %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n, fill = hashtag)) +
  geom_col(show.legend = TRUE) +
  facet_wrap(~sentiment, scales = "free_y") +
  labs(y = "Contribution to sentiment",x = NULL) +
  coord_flip()
tidytweet %>%
  inner_join(get_sentiments("bing")) %>%
  dplyr::count(word, sentiment, sort = TRUE) %>%
  acast(word ~ sentiment, value.var = "n", fill = 0) %>%
  comparison.cloud(colors = c("#F8766D", "#00BFC4"),
  max.words = 100,scale=c(3,.15))
```



Topic Modeling:
```{r}
ui = unique(totdtm$i)
totdtm_new = totdtm[ui,]
tweet_LDA <- LDA(totdtm_new, k=5, control = list(seed = 1234))
tweet_topics <- tidy(tweet_LDA, matrix = "beta")
tweet_top_terms <- tweet_topics %>% group_by(topic) %>% 
  top_n(15, beta) %>% ungroup() %>% arrange(topic, -beta)
tweet_top_terms %>% mutate(term = reorder(term, beta)) %>%
  ggplot(aes(term, beta, fill=factor(topic))) + 
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  coord_flip()
```



N-Grams:
```{r}
# Code for n-grams was adapted from:
# https://www.tidytextmining.com/
totaltweet$text <- iconv(totaltweet$text, "latin1", "ASCII", 
  sub = "")
totaltweet$text <- gsub("http(s?)([^ ]*)", " ",
  totaltweet$text, ignore.case = T)
totaltweet$text <- gsub("&amp", "and", totaltweet$text)
tweet_bigrams <- totaltweet %>% unnest_tokens(bigram, text,
  token = "ngrams", n = 2)
bigrams_separated <- tweet_bigrams %>% separate(bigram,
  c("word1","word2"), sep = " ")
bigrams_filtered <- bigrams_separated %>% 
  filter(!word1 %in% stop_words$word) %>%
  filter(!word2 %in% stop_words$word)
bigram_counts <- bigrams_filtered %>% 
  dplyr::count(word1, word2, sort = TRUE)
bigrams_united <- bigrams_filtered %>% unite(bigram, word1,
  word2, sep = " ")
bigrams_united_freq <- bigrams_united %>% dplyr::count(bigram, sort = TRUE)
tweet_trigrams <- totaltweet %>% unnest_tokens(trigram, text,
  token = "ngrams", n = 3)
trigrams_separated <- tweet_trigrams %>% separate(trigram,
  c("word1","word2","word3"), sep = " ")
trigrams_filtered <- trigrams_separated %>% 
  filter(!word1 %in% stop_words$word) %>%
  filter(!word2 %in% stop_words$word) %>%
  filter(!word3 %in% stop_words$word)
trigram_counts <- trigrams_filtered %>% 
  dplyr::count(word1, word2, word3, sort = TRUE)
trigrams_united <- trigrams_filtered %>% unite(trigram, word1,
  word2, word3, sep = " ")
trigrams_united_freq <- trigrams_united %>% dplyr::count(trigram,
  sort = TRUE)
tweet_4grams <- totaltweet %>% unnest_tokens(fourgram, text,
  token = "ngrams", n = 4)
fourgrams_separated <- tweet_4grams %>% separate(fourgram,
  c("word1","word2","word3","word4"), sep = " ")
fourgrams_filtered <- fourgrams_separated %>% 
  filter(!word1 %in% stop_words$word) %>%
  filter(!word2 %in% stop_words$word) %>%
  filter(!word3 %in% stop_words$word) %>%
  filter(!word4 %in% stop_words$word)
fourgram_counts <- fourgrams_filtered %>% 
  dplyr::count(word1, word2, word3, word4, sort = TRUE)
fourgrams_united <- fourgrams_filtered %>% unite(fourgram, word1,
  word2, word3, word4, sep = " ")
fourgrams_united_freq <- fourgrams_united %>%
  dplyr::count(fourgram, sort = TRUE)
bigram_tf_idf <- bigrams_united %>% 
  dplyr::count(hashtag, bigram) %>%
  bind_tf_idf(bigram, hashtag, n) %>%
  arrange(desc(tf_idf))
bigram_tf_idf %>%
  arrange(desc(tf_idf)) %>%
  group_by(hashtag) %>%
  top_n(12, tf_idf) %>%
  ungroup() %>%
  dplyr::mutate(bigram = reorder(bigram, tf_idf)) %>%
  ggplot(aes(bigram, tf_idf, fill = hashtag)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ hashtag, ncol = 2, scales = "free") +
  coord_flip() +
  labs(y = "tf-idf of bigram to hashtag",
  x = "")
trigram_tf_idf <- trigrams_united %>% 
  dplyr::count(hashtag, trigram) %>%
  bind_tf_idf(trigram, hashtag, n) %>%
  arrange(desc(tf_idf))
trigram_tf_idf %>%
  arrange(desc(tf_idf)) %>%
  group_by(hashtag) %>%
  top_n(12, tf_idf) %>%
  ungroup() %>%
  dplyr::mutate(trigram = reorder(trigram, tf_idf)) %>%
  ggplot(aes(trigram, tf_idf, fill = hashtag)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ hashtag, ncol = 2, scales = "free") +
  coord_flip() +
  labs(y = "tf-idf of trigram to hashtag",
  x = "")
fourgram_tf_idf <- fourgrams_united %>% 
  dplyr::count(hashtag, fourgram) %>%
  bind_tf_idf(fourgram, hashtag, n) %>%
  arrange(desc(tf_idf))
fourgram_tf_idf %>%
  arrange(desc(tf_idf)) %>%
  group_by(hashtag) %>%
  top_n(12, tf_idf) %>%
  ungroup() %>%
  dplyr::mutate(fourgram = reorder(fourgram, tf_idf)) %>%
  ggplot(aes(fourgram, tf_idf, fill = hashtag)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ hashtag, ncol = 2, scales = "free") +
  coord_flip() +
  labs(y = "tf-idf of 4-gram to hashtag",
  x = "")
```
LSA:
```{r}
tottdm_sparse <- removeSparseTerms(tottdm, sparse = 0.98)
tottdm_tfidf <- weightTfIdf(tottdm_sparse)
totlsa <- lsa(tottdm_tfidf, dims = 5)
totlsa_topics <- lsa(tottdm_tfidf, dims = 10)
which.max(totlsa_topics$tk[,1])
which.max(totlsa_topics$tk[,2])
which.max(totlsa_topics$tk[,3])
which.max(totlsa_topics$tk[,4])
which.max(totlsa_topics$tk[,5])
which.max(totlsa_topics$tk[,6])
which.max(totlsa_topics$tk[,7])
which.max(totlsa_topics$tk[,8])
which.max(totlsa_topics$tk[,9])
which.max(totlsa_topics$tk[,10])
docu <- as.data.frame(totlsa_topics$dk)
colnames(docu) <- c("TopicA", "TopicB", "TopicC", "TopicD",
  "TopicE", "TopicF", "TopicG", "TopicH", "TopicI", "TopicJ")
doc_topics <- as.data.frame(colnames(docu)[apply(docu,1,which.max)])
colnames(doc_topics) <- "Topic"
ggplot(doc_topics) + 
  geom_bar(aes(x = Topic, fill = Topic)) + 
  geom_text(aes( x = Topic, label = ..count..),
  stat = "count", vjust = -1) + ylim(-1000,30000)
term_topics <- as.data.frame(totlsa_topics$tk)
colnames(term_topics) <- c("TopicA", "TopicB", "TopicC", "TopicD",
  "TopicE", "TopicF", "TopicG", "TopicH", "TopicI", "TopicJ")
term_top_list <- 
  as.data.frame(colnames(term_topics)[apply(term_topics,
  1,which.max)])
colnames(term_top_list) <- "Topic"
term_topics$word <- row.names(term_topics)
term_topics <- term_topics %>% bind_cols(term_top_list)
term_topics1 <- term_topics %>% select(TopicA,word) %>% arrange(TopicA) %>% as.data.frame()
term_topics1 <- term_topics1[c(1:5),]
term_topics2 <- term_topics %>% select(TopicB,word) %>%
  arrange(TopicB) %>% as.data.frame()
term_topics2 <- term_topics2[c(1:5),]
term_topics3 <- term_topics %>% select(TopicC,word) %>%
  arrange(TopicC) %>% as.data.frame()
term_topics1 <- term_topics3[c(1:5),]
term_topics4 <- term_topics %>% select(TopicD,word) %>%
  arrange(TopicD) %>% as.data.frame()
term_topics4 <- term_topics4[c(1:5),]
term_topics5 <- term_topics %>% select(TopicE,word) %>%
  arrange(TopicE) %>% as.data.frame()
term_topics5 <- term_topics1[c(1:5),]
term_topics6 <- term_topics %>% select(TopicF,word) %>%
  arrange(TopicF) %>% as.data.frame()
term_topics6 <- term_topics1[c(1:5),]
```
Word Associations
```{r}

```



Clustering:
```{r}
set.seed(1234)
d <- dist(t(totlsa$tk), method="manhattan")
d <- as.matrix(d)
fviz_nbclust(d, pam, method = "wss") + theme_classic()
totlsa.tk <- as.data.frame(totlsa$tk)
totclust <- pam(totlsa.tk, 3, metric = "manhattan")
totclust$clustering
clusters <- as.data.frame(totclust$clustering)
clusters$word <- row.names(clusters)
colnames(clusters) <- c("cluster", "word")
hashtag_word_clust <- hashtag_words %>% inner_join(clusters)
hashtag_word_clust %>%
  group_by(cluster) %>% 
  top_n(10, n) %>% 
  ungroup %>%
  ggplot(aes(word, n, fill = factor(cluster))) +
  geom_col(show.legend = TRUE) +
  labs(x = NULL, y = "tf-idf") +
  facet_wrap(~cluster, ncol = 3, scales = "free") +
  coord_flip() + labs(y = "Frequency of Use", x = "Word",
  title = "Top 5 words per cluster", fill = "Cluster")
```



Classification:
```{r}

```
