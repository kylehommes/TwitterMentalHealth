---
title: "Untitled"
author: "Kyle Hommes"
date: "5/11/2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
Packages Needed:
```{r,echo=TRUE,warning=FALSE,message=FALSE,,error=FALSE}
library(plyr)
library(dplyr)
library(ggplot2)
library(wordcloud)
library(wordcloud2)
library(twitteR)
library(webshot)
library(htmlwidgets)
library(data.table)
library(tm)
library(tidyr)
library(RColorBrewer)
library(tidytext)
library(stringr)
library(reshape2)
library(topicmodels)
library(RTextTools)
library(ggmap)
library(topicmodels)
library(lsa)
library(SnowballC)
library(scatterplot3d)
library(LSAfun)
library(maps)
library(cluster)
library(NbClust)
library(factoextra)
library(gridExtra)
library(caret)
library(class)
library(klaR)
library(e1071)
library(textstem)
library(shiny)
library(rsconnect)
library(knitr)
```
```{r wordcloud,include=FALSE}
# Code to include wordcloud in .pdf knit
my_graph=wordcloud2(demoFreq, size=1.5)
saveWidget(my_graph,"tmp.html",selfcontained = F)
webshot("tmp.html","wc1.png", delay =15, vwidth = 2000, vheight=2000)
```



Exploratory Data Analysis:
```{r,error=FALSE,warning=FALSE,message=FALSE}
wordcloud2(mhafreq.df)
wordcloud2(afreq.df)
wordcloud2(pfreq.df)
wordcloud2(sfreq.df)
wordcloud2(dfreq.df)
wordcloud2(totfreq.df)
wordcloud(mhafreq.df$word,mhafreq.df$freq,max.words=100,colors=brewer.pal(8,"Dark2"),scale=c(3,0.5),random.order=F)
ggplot(totaltweet) + geom_bar(aes(x = isRetweet, fill = isRetweet),
  stat = 'count') + geom_text(stat = 'count', 
  aes(x = isRetweet,label=..count..), vjust = 3) + 
  labs(x = "Retweeted Tweets", y = "Count", 
  fill = "Retweeted", 
  title = "Retweeted and Non-Retweeted\nTweets for All Hashtags")
ggplot(totaltweet) + geom_bar(aes(x = isRetweet, fill = hashtag)) + 
  facet_wrap(~hashtag) + geom_text(stat = 'count',
  aes(x = isRetweet, label=..count..), vjust = 1) +
  labs(x = "Retweeted Tweets", y = "Count", fill = "Hashtag",
  title = "Retweeted and Non-Retweeted\nTweets per Hashtag")
worldMap <- map_data("world")
loc_plot <- ggplot(worldMap)
loc_plot + geom_path(aes(x = long, y = lat, group = group)) +
  geom_point(data = location,aes(x=lon,y=lat),color="RED",
  size=.25) + labs(x = "Longitude", y = "Latitude", 
  title = "Map of Location for #mentalhealthawarness Tweets")
totaltweet %>%
  ungroup() %>% filter(!str_detect(text, "^RT")) %>%
  mutate(text = str_replace_all(text, replace_reg, "")) %>%
  unnest_tokens(word, text, token = "regex", 
  pattern = unnest_reg) %>%
  filter(!word %in% stop_words$word, 
  str_detect(word, "[a-z]")) %>%
  dplyr::count(word) %>%
  with(wordcloud(word, n, max.words = 100,
  colors=brewer.pal(8,"Dark2"), 
  scale=c(1.5,0.3), random.order=F))
hashtag_words <- totaltweet %>% 
  filter(!str_detect(text, "^RT")) %>%
  mutate(text = str_replace_all(text, replace_reg, "")) %>%
  unnest_tokens(word, text, token = "regex", 
  pattern = unnest_reg) %>%
  filter(!word %in% stop_words$word,
  str_detect(word, "[a-z]")) %>% 
  dplyr::count(hashtag, word, sort = TRUE) %>% ungroup()
total_words <- hashtag_words %>% group_by(hashtag) %>%
  dplyr::summarize(total = sum(n))
hashtag_words <- left_join(hashtag_words,total_words)
ggplot(hashtag_words, aes(n/total, fill = hashtag)) +
  geom_histogram(show.legend = FALSE) +
  xlim(NA, 0.0009) +
  facet_wrap(~hashtag, ncol = 2, scales = "free_y") +
  labs(x = "Percentage of Total Word Usage", y = "Frequency", 
  title = "Word Frequency Distribution per Hashtag")
hashtag_tfidf <- hashtag_words %>% bind_tf_idf(word, hashtag, n)
hashtag_tfidf <- hashtag_tfidf %>% dplyr::select(-total) %>%
  arrange(desc(tf_idf))
hashtag_tfidf %>%
  arrange(desc(tf_idf)) %>%
  mutate(word = factor(word, levels = rev(unique(word)))) %>% 
  group_by(hashtag) %>% 
  top_n(15) %>% 
  ungroup %>%
  ggplot(aes(word, tf_idf, fill = hashtag)) +
  geom_col(show.legend = FALSE) +
  labs(x = NULL, y = "tf-idf") +
  facet_wrap(~hashtag, ncol = 2, scales = "free") +
  coord_flip() + labs(y = "Frequency of Use", x = "Word",
  title = "Most Used Words by Hashtag")
freq_by_rank <- hashtag_words %>% 
  group_by(hashtag) %>% 
  mutate(rank = row_number(), `term frequency` = n/total)
freq_by_rank %>% 
  ggplot(aes(rank, `term frequency`, color = hashtag)) + 
  geom_line(size = 1.1, alpha = 0.8, show.legend = TRUE) + 
  scale_x_log10() + scale_y_log10() + labs(x = "Rank", 
  y = "Term Frequency", 
  title = "Zipf's Plot of each Hashtag", color = "Hashtag")
grid.arrange(cluster_plot,lda_plot,lsa_plot, nrow = 2, ncol = 2)
```
Sentiment Analysis:
```{r,error=FALSE,message=FALSE,warning=FALSE}
# Sentiment analysis code adapted from:
# https://www.tidytextmining.com/
tweet_sentiment <- tidytweet %>%
  inner_join(get_sentiments("bing")) %>%
  dplyr::count(hashtag, index = linenumber %/% 50,sentiment) %>%
  spread(sentiment, n, fill = 0) %>%
  mutate(sentiment = positive - negative)
tweet_sentiment_nrc <- tidytweet %>%
  inner_join(get_sentiments("nrc")) %>%
  group_by(index = linenumber %/% 50, hashtag, sentiment) %>%
  summarize(n = n())
ggplot(tweet_sentiment_nrc) + 
  geom_bar(aes(x = index, y = n, fill = hashtag), 
  stat = "identity") + facet_wrap(~sentiment, ncol = 5)
ggplot(tweet_sentiment,aes(index, sentiment,fill = hashtag)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~hashtag, ncol = 2, scales = "free_x")
bing_word_counts <- tidytweet %>%
  inner_join(get_sentiments("bing")) %>%
  dplyr::count(word, sentiment, sort = TRUE) %>%
  ungroup()
bing_word_hashtag_counts <- tidytweet %>%
  inner_join(get_sentiments("bing")) %>%
  dplyr::count(word, hashtag, sentiment, sort = TRUE) %>%
  ungroup()
bing_word_counts %>%
  group_by(sentiment) %>%
  top_n(15) %>%
  ungroup() %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n, fill = sentiment)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~sentiment, scales = "free_y") +
  labs(y = "Contribution to sentiment",x = NULL) +
  coord_flip()
bing_word_hashtag_counts %>%
  group_by(hashtag,sentiment) %>%
  top_n(5) %>%
  ungroup() %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n, fill = sentiment)) +
  geom_col(show.legend = TRUE) +
  facet_wrap(~hashtag, scales = "free_y") +
  labs(y = "Contribution to sentiment",x = NULL) +
  coord_flip()
bing_word_counts_nrc <- tidytweet %>%
  inner_join(get_sentiments("nrc")) %>%
  dplyr::count(word, sentiment, sort = TRUE) %>%
  ungroup()
bing_word_counts_nrc %>%
  group_by(sentiment) %>%
  top_n(10) %>%
  ungroup() %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n, fill = sentiment)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~sentiment, scales = "free_y") +
  labs(y = "Contribution to sentiment",x = NULL) +
  coord_flip()
bing_word_hashtag_counts_nrc <- tidytweet %>%
  inner_join(get_sentiments("nrc")) %>%
  dplyr::count(word, hashtag, sentiment, sort = TRUE) %>%
  ungroup()
bing_word_hashtag_counts_nrc %>%
  group_by(hashtag,sentiment) %>%
  top_n(2) %>%
  ungroup() %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n, fill = hashtag)) +
  geom_col(show.legend = TRUE) +
  facet_wrap(~sentiment, scales = "free_y") +
  labs(y = "Contribution to sentiment",x = NULL) +
  coord_flip()
tidytweet %>%
  inner_join(get_sentiments("bing")) %>%
  dplyr::count(word, sentiment, sort = TRUE) %>%
  acast(word ~ sentiment, value.var = "n", fill = 0) %>%
  comparison.cloud(colors = c("#F8766D", "#00BFC4"),
  max.words = 100,scale=c(3,.15))
```



Topic Modeling:
```{r}
ui = unique(totdtm$i)
totdtm_new = totdtm[ui,]
tweet_LDA <- LDA(totdtm_new, k=5, control = list(seed = 1234))
tweet_topics <- tidy(tweet_LDA, matrix = "beta")
tweet_top_terms <- tweet_topics %>% group_by(topic) %>% 
  top_n(15, beta) %>% ungroup() %>% arrange(topic, -beta)
tweet_top_terms %>% mutate(term = reorder(term, beta)) %>%
  ggplot(aes(term, beta, fill=factor(topic))) + 
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  coord_flip() + labs(x = "Words", 
  y = "Frequency of Use in Each Topic", 
  title = "Top 15 Words in Each Topic")
lda_topics <- tweet_topics %>% group_by(term) %>% summarize(beta = max(beta))
lda_tweet_topics <- lda_topics %>% inner_join(tweet_topics)
colnames(lda_tweet_topics)[1] <- "word"
combo <- tidytweet[,c(16,18)]
lda_combo <- combo %>% inner_join(lda_tweet_topics)
lda_plot <- ggplot(lda_combo) + 
  geom_bar(aes(x = factor(topic), fill = factor(topic)), 
  show.legend = FALSE) + 
  geom_text(aes( x = factor(topic), label = ..count..),
  stat = "count", vjust = -1) + ylim(-1000,30000) +
  labs(x = "Topic", y = "Count", 
  title = "Number of Words most associated 
  with Each LDA Topic by hashtag") +
  facet_wrap(~hashtag, scales = "free")
lda_plot
```



N-Grams:
```{r}
# Code for n-grams was adapted from:
# https://www.tidytextmining.com/
totaltweet$text <- iconv(totaltweet$text, "latin1", "ASCII", 
  sub = "")
totaltweet$text <- gsub("http(s?)([^ ]*)", " ",
  totaltweet$text, ignore.case = T)
totaltweet$text <- gsub("&amp", "and", totaltweet$text)
tweet_bigrams <- totaltweet %>% unnest_tokens(bigram, text,
  token = "ngrams", n = 2)
bigrams_separated <- tweet_bigrams %>% separate(bigram,
  c("word1","word2"), sep = " ")
bigrams_filtered <- bigrams_separated %>% 
  filter(!word1 %in% stop_words$word) %>%
  filter(!word2 %in% stop_words$word)
bigram_counts <- bigrams_filtered %>% 
  dplyr::count(word1, word2, sort = TRUE)
bigrams_united <- bigrams_filtered %>% unite(bigram, word1,
  word2, sep = " ")
bigrams_united_freq <- bigrams_united %>% dplyr::count(bigram, sort = TRUE)
tweet_trigrams <- totaltweet %>% unnest_tokens(trigram, text,
  token = "ngrams", n = 3)
trigrams_separated <- tweet_trigrams %>% separate(trigram,
  c("word1","word2","word3"), sep = " ")
trigrams_filtered <- trigrams_separated %>% 
  filter(!word1 %in% stop_words$word) %>%
  filter(!word2 %in% stop_words$word) %>%
  filter(!word3 %in% stop_words$word)
trigram_counts <- trigrams_filtered %>% 
  dplyr::count(word1, word2, word3, sort = TRUE)
trigrams_united <- trigrams_filtered %>% unite(trigram, word1,
  word2, word3, sep = " ")
trigrams_united_freq <- trigrams_united %>% dplyr::count(trigram,
  sort = TRUE)
tweet_4grams <- totaltweet %>% unnest_tokens(fourgram, text,
  token = "ngrams", n = 4)
fourgrams_separated <- tweet_4grams %>% separate(fourgram,
  c("word1","word2","word3","word4"), sep = " ")
fourgrams_filtered <- fourgrams_separated %>% 
  filter(!word1 %in% stop_words$word) %>%
  filter(!word2 %in% stop_words$word) %>%
  filter(!word3 %in% stop_words$word) %>%
  filter(!word4 %in% stop_words$word)
fourgram_counts <- fourgrams_filtered %>% 
  dplyr::count(word1, word2, word3, word4, sort = TRUE)
fourgrams_united <- fourgrams_filtered %>% unite(fourgram, word1,
  word2, word3, word4, sep = " ")
fourgrams_united_freq <- fourgrams_united %>%
  dplyr::count(fourgram, sort = TRUE)
bigram_tf_idf <- bigrams_united %>% 
  dplyr::count(hashtag, bigram) %>%
  bind_tf_idf(bigram, hashtag, n) %>%
  arrange(desc(tf_idf))
bigram_tf_idf %>%
  arrange(desc(tf_idf)) %>%
  group_by(hashtag) %>%
  top_n(12, tf_idf) %>%
  ungroup() %>%
  dplyr::mutate(bigram = reorder(bigram, tf_idf)) %>%
  ggplot(aes(bigram, tf_idf, fill = hashtag)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ hashtag, ncol = 2, scales = "free") +
  coord_flip() +
  labs(y = "tf-idf of bigram to hashtag",
  x = "")
trigram_tf_idf <- trigrams_united %>% 
  dplyr::count(hashtag, trigram) %>%
  bind_tf_idf(trigram, hashtag, n) %>%
  arrange(desc(tf_idf))
trigram_tf_idf %>%
  arrange(desc(tf_idf)) %>%
  group_by(hashtag) %>%
  top_n(12, tf_idf) %>%
  ungroup() %>%
  dplyr::mutate(trigram = reorder(trigram, tf_idf)) %>%
  ggplot(aes(trigram, tf_idf, fill = hashtag)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ hashtag, ncol = 2, scales = "free") +
  coord_flip() +
  labs(y = "tf-idf of trigram to hashtag",
  x = "")
fourgram_tf_idf <- fourgrams_united %>% 
  dplyr::count(hashtag, fourgram) %>%
  bind_tf_idf(fourgram, hashtag, n) %>%
  arrange(desc(tf_idf))
fourgram_tf_idf %>%
  arrange(desc(tf_idf)) %>%
  group_by(hashtag) %>%
  top_n(12, tf_idf) %>%
  ungroup() %>%
  dplyr::mutate(fourgram = reorder(fourgram, tf_idf)) %>%
  ggplot(aes(fourgram, tf_idf, fill = hashtag)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ hashtag, ncol = 2, scales = "free") +
  coord_flip() +
  labs(y = "tf-idf of 4-gram to hashtag",
  x = "")
```
LSA:
```{r,message=FALSE,warning=FALSE,error=FALSE}
tottdm_sparse_lsa <- removeSparseTerms(tottdm, sparse = 0.99)
tottdm_tfidf_lsa <- weightTfIdf(tottdm_sparse_lsa)
totlsa_topics <- lsa(tottdm_tfidf_lsa, dims = 10)
which.max(totlsa_topics$tk[,1])
which.max(totlsa_topics$tk[,2])
which.max(totlsa_topics$tk[,3])
which.max(totlsa_topics$tk[,4])
which.max(totlsa_topics$tk[,5])
which.max(totlsa_topics$tk[,6])
which.max(totlsa_topics$tk[,7])
which.max(totlsa_topics$tk[,8])
which.max(totlsa_topics$tk[,9])
which.max(totlsa_topics$tk[,10])
docu <- as.data.frame(totlsa_topics$dk)
colnames(docu) <- c("TopicA", "TopicB", "TopicC", "TopicD",
  "TopicE", "TopicF", "TopicG", "TopicH", "TopicI", "TopicJ")
doc_topics <- as.data.frame(colnames(docu)[apply(docu,1,which.max)])
colnames(doc_topics) <- "Topic"
df.add <- data.frame(Topic = "TopicE", n = 0)
doc_topics_count <- doc_topics %>% dplyr::count(Topic) %>%
  as.data.frame() %>% bind_rows(df.add)
ggplot(doc_topics_count) + 
  geom_bar(aes(x = factor(Topic), y = n, fill = factor(Topic)), 
  stat = "identity", show.legend = FALSE) + 
  geom_text(aes( x = factor(Topic), y = n, label = n),
  stat = "identity", vjust = -1) + ylim(-1000,40000) +
  labs(x = "Topic", y = "Count", 
  title = "Number of Tweets most associated with Each LSA Topic")
hashtag_tweet <- data.frame(hashtag = totaltweet$hashtag)
hashtag_tweet$Tweet <- rownames(hashtag_tweet)
doc_topics$Tweet <- rownames(doc_topics)
tweet_combo_lsa <- hashtag_tweet %>% inner_join(doc_topics)
ggplot(tweet_combo_lsa) + 
  geom_bar(aes(x = factor(Topic), fill = factor(Topic)), 
  show.legend = FALSE) + 
  geom_text(aes( x = factor(Topic), label = ..count..),
  stat = "count", vjust = -1) + ylim(-1000,15000) +
  labs(x = "Topic", y = "Count", 
  title = "Number of Tweets most associated 
  with Each LSA Topic by Hashtag") +
  facet_wrap(~hashtag, scales = "free")
term_topics <- as.data.frame(totlsa_topics$tk)
colnames(term_topics) <- c("TopicA", "TopicB", "TopicC", "TopicD",
  "TopicE", "TopicF", "TopicG", "TopicH", "TopicI", "TopicJ")
term_top_list <- 
  as.data.frame(colnames(term_topics)[apply(term_topics,
  1,which.max)])
colnames(term_top_list) <- "Topic"
term_topics$word <- row.names(term_topics)
term_topics <- term_topics %>% bind_cols(term_top_list)
term_topics1 <- term_topics %>% dplyr::select(TopicA,word) %>%
  arrange(desc(TopicA)) %>% mutate(label = "TopicA",
  freq = TopicA) %>% as.data.frame()
term_topics1 <- term_topics1[c(1:5),]
term_topics2 <- term_topics %>% dplyr::select(TopicB,word) %>%
  arrange(desc(TopicB)) %>% mutate(label = "TopicB",
  freq = TopicB) %>% as.data.frame()
term_topics2 <- term_topics2[c(1:5),]
term_topics3 <- term_topics %>% dplyr::select(TopicC,word) %>%
  arrange(desc(TopicC)) %>% mutate(label = "TopicC",
  freq = TopicC) %>% as.data.frame()
term_topics3 <- term_topics3[c(1:5),]
term_topics4 <- term_topics %>% dplyr::select(TopicD,word) %>%
  arrange(desc(TopicD)) %>% mutate(label = "TopicD",
  freq = TopicD) %>% as.data.frame()
term_topics4 <- term_topics4[c(1:5),]
term_topics5 <- term_topics %>% dplyr::select(TopicE,word) %>%
  arrange(desc(TopicE)) %>% mutate(label = "TopicE",
  freq = TopicE) %>% as.data.frame()
term_topics5 <- term_topics5[c(1:5),]
term_topics6 <- term_topics %>% dplyr::select(TopicF,word) %>%
  arrange(desc(TopicF)) %>% mutate(label = "TopicF",
  freq = TopicF) %>% as.data.frame()
term_topics6 <- term_topics6[c(1:5),]
term_topics7 <- term_topics %>% dplyr::select(TopicG,word) %>%
  arrange(desc(TopicG)) %>% mutate(label = "TopicG",
  freq = TopicG) %>% as.data.frame()
term_topics7 <- term_topics7[c(1:5),]
term_topics8 <- term_topics %>% dplyr::select(TopicH,word) %>%
  arrange(desc(TopicH)) %>% mutate(label = "TopicH",
  freq = TopicH) %>% as.data.frame()
term_topics8 <- term_topics8[c(1:5),]
term_topics9 <- term_topics %>% dplyr::select(TopicI,word) %>%
  arrange(desc(TopicI)) %>% mutate(label = "TopicI",
  freq = TopicI) %>% as.data.frame()
term_topics9 <- term_topics9[c(1:5),]
term_topics10 <- term_topics %>% dplyr::select(TopicJ,word) %>%
  arrange(desc(TopicJ)) %>% mutate(label = "TopicJ",
  freq = TopicJ) %>% as.data.frame()
term_topics10 <- term_topics10[c(1:5),]
term_topics_tot <- bind_rows(term_topics1,term_topics2,term_topics3,
  term_topics4,term_topics5,term_topics6,term_topics7,term_topics8,
  term_topics9,term_topics10)
ggplot(term_topics_tot) + geom_bar(aes(x = word, y = freq, 
  fill = factor(label)), stat = "identity", show.legend = FALSE) +
  facet_wrap(~label,scales = "free") +
  coord_flip() + labs(x = "Words", y = "LSA Similarity Score", 
  title = "Top 5 Words per LSA Topic")
term_topics_2 <- as.data.frame(totlsa_topics$tk)
colnames(term_topics_2) <- c("1", "2", "3", "4", 
  "5", "6", "7", "8", "9", "10")
term_top_list_2 <- 
  as.data.frame(colnames(term_topics_2)[apply(term_topics_2,
  1,which.max)])
colnames(term_top_list_2) <- "Topic"
term_topics_2$word <- row.names(term_topics_2)
term_topics_2 <- term_topics_2 %>% bind_cols(term_top_list_2)
term_combo_lsa <- term_topics %>% inner_join(hashtag_words)
term_combo_lsa_2 <- term_topics_2 %>% inner_join(hashtag_words)
ggplot(term_combo_lsa) + geom_bar(aes(x = factor(Topic), 
  fill = factor(Topic)), show.legend = FALSE) +
  geom_text(aes(x = factor(Topic), label = ..count..),
  stat = "count", vjust = -1) + labs(x = "LSA Topic", 
  y = "Number of Words", 
  title = "Number of Words most associated 
  with each LSA topic per hashtag") +
  facet_wrap(~hashtag, scales = "free") + ylim(0, 25)
lsa_plot <- ggplot(term_combo_lsa_2) + geom_bar(aes(x = factor(Topic), 
  fill = factor(Topic)), show.legend = FALSE) +
  geom_text(aes(x = factor(Topic), label = ..count..),
  stat = "count", vjust = -1) + labs(x = "LSA Topic", 
  y = "Number of Words", 
  title = "Number of Words most associated 
  with each LSA topic by hashtag") +
  facet_wrap(~hashtag, scales = "free") + ylim(0, 30)
mhatdm_sparse_lsa <- removeSparseTerms(mhatdm, sparse = 0.99)
mhatdm_tfidf_lsa <- weightTfIdf(mhatdm_sparse_lsa)
mhalsa_topics <- lsa(mhatdm_tfidf_lsa, dims = 10)
docu_mha <- as.data.frame(mhalsa_topics$dk)
colnames(docu_mha) <- c("TopicA", "TopicB", "TopicC", "TopicD",
  "TopicE", "TopicF", "TopicG", "TopicH", "TopicI", "TopicJ")
doc_topics_mha <- as.data.frame(colnames(docu_mha)[apply(docu_mha,1,
  which.max)])
colnames(doc_topics_mha) <- "Topic"
df.add_mha <- data.frame(Topic = "TopicB", n = 0)
doc_topics_count_mha <- doc_topics_mha %>% dplyr::count(Topic) %>%
  as.data.frame() %>% bind_rows(df.add_mha)
ggplot(doc_topics_count_mha) + 
  geom_bar(aes(x = factor(Topic), y = n, fill = factor(Topic)), 
  stat = "identity", show.legend = FALSE) + 
  geom_text(aes( x = factor(Topic), y = n, label = n),
  stat = "identity", vjust = -1) + ylim(-1000,40000) +
  labs(x = "Topic", y = "Count",
  title = "Number of Tweets most associated with Each 
  LSA Topic for #mentalhealthawarness")
term_topics_m <- as.data.frame(totlsa_topics_m$tk)
colnames(term_topics_m) <- c("TopicA", "TopicB", "TopicC", "TopicD",
  "TopicE", "TopicF", "TopicG", "TopicH", "TopicI", "TopicJ")
term_top_list_m <- 
  as.data.frame(colnames(term_topics_m)[apply(term_topics_m,
  1,which.max)])
colnames(term_top_list_m) <- "Topic"
term_topics_m$word <- row.names(term_topics_m)
term_topics_m <- term_topics_m %>% bind_cols(term_top_list)
term_topics1_m <- term_topics_m %>% select(TopicA,word) %>%
  arrange(desc(TopicA)) %>% mutate(label = "TopicA",
  freq = TopicA) %>% as.data.frame()
term_topics1_m <- term_topics1_m[c(1:5),]
term_topics2_m <- term_topics_m %>% select(TopicB,word) %>%
  arrange(desc(TopicB)) %>% mutate(label = "TopicB",
  freq = TopicB) %>% as.data.frame()
term_topics2_m <- term_topics2_m[c(1:5),]
term_topics3 <- term_topics %>% select(TopicC,word) %>%
  arrange(desc(TopicC)) %>% mutate(label = "TopicC",
  freq = TopicC) %>% as.data.frame()
term_topics3 <- term_topics3[c(1:5),]
term_topics4 <- term_topics %>% select(TopicD,word) %>%
  arrange(desc(TopicD)) %>% mutate(label = "TopicD",
  freq = TopicD) %>% as.data.frame()
term_topics4 <- term_topics4[c(1:5),]
term_topics5 <- term_topics %>% select(TopicE,word) %>%
  arrange(desc(TopicE)) %>% mutate(label = "TopicE",
  freq = TopicE) %>% as.data.frame()
term_topics5 <- term_topics5[c(1:5),]
term_topics6 <- term_topics %>% select(TopicF,word) %>%
  arrange(desc(TopicF)) %>% mutate(label = "TopicF",
  freq = TopicF) %>% as.data.frame()
term_topics6 <- term_topics6[c(1:5),]
term_topics7 <- term_topics %>% select(TopicG,word) %>%
  arrange(desc(TopicG)) %>% mutate(label = "TopicG",
  freq = TopicG) %>% as.data.frame()
term_topics7 <- term_topics7[c(1:5),]
term_topics8 <- term_topics %>% select(TopicH,word) %>%
  arrange(desc(TopicH)) %>% mutate(label = "TopicH",
  freq = TopicH) %>% as.data.frame()
term_topics8 <- term_topics8[c(1:5),]
term_topics9 <- term_topics %>% select(TopicI,word) %>%
  arrange(desc(TopicI)) %>% mutate(label = "TopicI",
  freq = TopicI) %>% as.data.frame()
term_topics9 <- term_topics9[c(1:5),]
term_topics10 <- term_topics %>% select(TopicJ,word) %>%
  arrange(desc(TopicJ)) %>% mutate(label = "TopicJ",
  freq = TopicJ) %>% as.data.frame()
term_topics10 <- term_topics10[c(1:5),]
term_topics_tot <- bind_rows(term_topics1,term_topics2,term_topics3,
  term_topics4,term_topics5,term_topics6,term_topics7,term_topics8,
  term_topics9,term_topics10)
ggplot(term_topics_tot) + geom_bar(aes(x = word, y = freq, 
  fill = factor(label)), stat = "identity", show.legend = FALSE) +
  facet_wrap(~label,scales = "free") +
  coord_flip() + labs(x = "Words", y = "LSA Similarity Score", 
  title = "Top 5 Words per LSA Topic")
```
Word Associations
```{r}

```



Clustering:
```{r}
set.seed(1234)
tottdm_sparse <- removeSparseTerms(tottdm, sparse = 0.999)
tottdm_tfidf <- weightTfIdf(tottdm_sparse)
totlsa_topics_clust <- lsa(tottdm_tfidf, dims = 10)
d <- dist(t(totlsa_topics_clust$tk), method="manhattan")
d <- data.table(d)
fviz_nbclust(d, pam, k.max = 15, method = "wss") +
  theme_classic() + geom_vline(xintercept = 3, linetype = 2)
totlsa.tk <- as.data.frame(totlsa$tk)
totclust <- pam(totlsa.tk, 3, metric = "manhattan")
head(totclust$clustering, 10)
clusters <- as.data.frame(totclust$clustering)
clusters$word <- row.names(clusters)
colnames(clusters) <- c("cluster", "word")
hashtag_word_clust <- hashtag_words %>% inner_join(clusters)
hashtag_word_clust %>%
  group_by(cluster) %>% 
  top_n(10, n) %>% 
  ungroup %>%
  ggplot(aes(word, n, fill = factor(cluster))) +
  geom_col(show.legend = TRUE) +
  labs(x = NULL, y = "tf-idf") +
  facet_wrap(~cluster, ncol = 3, scales = "free") +
  coord_flip() + labs(y = "Frequency of Use", x = "Word",
  title = "Top 5 words per cluster", fill = "Cluster")
cluster_plot <- ggplot(hashtag_word_clust) + 
  geom_bar(aes(x = factor(cluster), 
  fill = factor(cluster)), show.legend = FALSE) +
  geom_text(aes(x = factor(cluster), label = ..count..),
  stat = "count", vjust = -1) + labs(x = "Cluster", 
  y = "Number of Words", title = "Number of Words per Cluster") +
  facet_wrap(~hashtag, scales = "free") + ylim(0, 1000)
cluster_plot
```



Classification:
```{r}
set.seed(1234)
totaltweet_random <- sample_frac(totaltweet, 1L)
tottxt_ran <- totaltweet_random$text[1:1500]
tottxt_ran <- iconv(tottxt_ran, "latin1", "ASCII", sub = "")
tottxt_ran <- gsub("http(s?)([^ ]*)", " ", tottxt_ran, ignore.case = T)
tottxt_ran <- gsub("&amp", "and", tottxt_ran)
totcorpus_ran <- VCorpus(VectorSource(tottxt_ran))
totcorpus_ran <- tm_map(totcorpus_ran,
  content_transformer(lemmatize_strings))
totcorpus_ran <- tm_map(totcorpus_ran, PlainTextDocument)
totdtm_ran <- DocumentTermMatrix(x= totcorpus_ran, control= ctrl)
totdtm_ran <- removeSparseTerms(totdtm_ran, sparse = .99)
train <- totdtm_ran[1:1000,]
train.labels <- totaltweet_random$isRetweet[1:1000]
test <- totdtm_ran[1001:1500,]
test.labels <- totaltweet_random$isRetweet[1001:1500]
train <- as.data.frame(as.matrix(train))
test <- as.data.frame(as.matrix(test))
train.labels <- as.factor(train.labels)
test.labels <- as.factor(test.labels)
nb_model <- naiveBayes(train, train.labels)
nb_pred <- predict(nb_model, test)
table(nb_pred, test.labels)
confusionMatrix(table(nb_pred, test.labels))
nb_cm <- confusionMatrix(table(nb_pred, test.labels))
svm_model <- svm(train, train.labels, kernal = "linear")
svm_pred <- predict(svm_model, test)
table(svm_pred, test.labels)
confusionMatrix(table(svm_pred, test.labels))
svm_cm <- confusionMatrix(table(svm_pred, test.labels))
fourfoldplot(svm_cm$table, color = c("#99CC99", "#CC6666"),
  conf.level = 0, margin = 1, main = "Confusion Matrix")
fourfoldplot(nb_cm$table, color = c("#CC6666", "#99CC99"),
  conf.level = 0, margin = 1, main = "Confusion Matrix")
```



Search Function
```{r, warning=FALSE}
# Code adapted from http://rstudio-pubs-static.s3.amazonaws.com/266142_e947ad96bead4abdb3d0fa8a539f7511.html
tweet_search <- totaltweet %>% dplyr::mutate(rowIndex=
  as.numeric(row.names(.))) %>%
  dplyr::select(retweetCount,rowIndex,hashtag)
tweet_search$text <- tottxt
docList <- as.list(tweet_search$text)
N.docs <- length(docList)
QrySearch <- function(queryTerm) {
  my.docs <- VectorSource(c(docList, queryTerm))
  my.corpus <- VCorpus(my.docs) %>% 
    tm_map(stemDocument) %>%
    tm_map(removeNumbers) %>% 
    tm_map(content_transformer(tolower)) %>% 
    tm_map(removeWords,stopwords("en")) %>%
    tm_map(stripWhitespace)
  term.doc.matrix.stm <- TermDocumentMatrix(my.corpus,
    control=list(weighting=function(x) weightSMART(x,spec="ltc"),
    wordLengths=c(1,Inf)))
  term.doc.matrix <- tidy(term.doc.matrix.stm) %>% 
    dplyr::group_by(document) %>% 
    dplyr::mutate(vtrLen=sqrt(sum(count^2))) %>% 
    dplyr::mutate(count=count/vtrLen) %>% 
    ungroup() %>% 
    dplyr::select(term:count)
  docMatrix <- term.doc.matrix %>% 
    dplyr::mutate(document=as.numeric(document)) %>% 
    dplyr::filter(document<N.docs+1)
  qryMatrix <- term.doc.matrix %>% 
    dplyr::mutate(document=as.numeric(document)) %>% 
    dplyr::filter(document>=N.docs+1)
  searchRes <- docMatrix %>% 
    inner_join(qryMatrix,by=c("term"="term"),
    suffix=c(".doc",".query")) %>% 
    dplyr::mutate(termScore=round(count.doc*count.query,4)) %>% 
    dplyr::group_by(document.query,document.doc) %>% 
    dplyr::summarise(Score=sum(termScore)) %>% 
    filter(row_number(desc(Score))<=10) %>% 
    dplyr::arrange(desc(Score)) %>% 
    left_join(tweet_search,by=c("document.doc"="rowIndex")) %>% 
    ungroup() %>% 
    rename(Result=text) %>% 
    dplyr::select(Result,Score,retweetCount,hashtag) %>% 
    data.frame()
  return(searchRes)
}
support_search <- QrySearch("support")
support_search
love_search <- QrySearch("love")
love_search
stress_search <- QrySearch("stress")
stress_search
son_search <- QrySearch("son")
son_search
help_search <- QrySearch("help")
help_search
```



```{r}

```