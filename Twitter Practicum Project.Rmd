---
title: "Untitled"
author: "Kyle Hommes"
date: "5/11/2018"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
Packages Needed:
```{r,echo=TRUE,warning=FALSE,message=FALSE,,error=FALSE}
library(plyr)
library(dplyr)
library(ggplot2)
library(wordcloud)
library(wordcloud2)
library(twitteR)
library(webshot)
library(htmlwidgets)
library(data.table)
library(tm)
library(tidyr)
library(RColorBrewer)
library(tidytext)
library(stringr)
library(reshape2)
library(topicmodels)
library(RTextTools)
library(ggmap)
library(topicmodels)
library(lsa)
library(SnowballC)
library(scatterplot3d)
library(LSAfun)
library(maps)
```
```{r wordcloud,include=FALSE}
# Code to include wordcloud in .pdf knit
my_graph=wordcloud2(demoFreq, size=1.5)
saveWidget(my_graph,"tmp.html",selfcontained = F)
webshot("tmp.html","wc1.png", delay =15, vwidth = 2000, vheight=2000)
```
Setup Twitter API:
```{r}
setup_twitter_oauth(consumer_key = consumer.key, 
  consumer_secret = consumer.secret, access_token = access.token,
  access_secret = access.secret)
```



Pull Tweets for the following hashtags:

Code for extracting the tweets is in Twitter Pulls.R. The hashtags pulled were:

#mentalhealthawareness
#anxiety
#depression
#ptsd
#suicide
```{r}
```
Clean the Data:
```{r}
mhatweet <- twListToDF(mentalhealthawarenesstweets)
anxtweet <- twListToDF(anxietytweets)
deptweet <- twListToDF(depressiontweets)
ptsdtweet <- twListToDF(ptsdtweets)
suitweet <- twListToDF(suicidetweets)
totaltweet <- bind_rows(mhatweet %>% mutate(hashtag = "MHA"),
  anxtweet %>% mutate(hashtag = "Anxiety"), deptweet %>%
  mutate(hashtag = "Depression"), ptsdtweet %>% 
  mutate(hashtag = "PTSD"), suitweet %>% 
  mutate(hashtag = "Suicide"))

# Cleaned the tweets for tidy analysis with code adapted from 
# https://www.tidytextmining.com
replace_reg <- "https://t.co/[A-Za-z\\d]+|http://[A-Za-z\\d]+|&amp;|&lt;|&gt;|RT|https"
unnest_reg <- "([^A-Za-z_\\d#@']|'(?![A-Za-z_\\d#@]))"
tidytweet <- totaltweet %>%
  group_by(hashtag) %>% 
  dplyr::mutate(linenumber = row_number()) %>%
  ungroup() %>% filter(!str_detect(text, "^RT")) %>%
  mutate(text = str_replace_all(text, replace_reg, "")) %>%
  unnest_tokens(word, text, token = "regex", 
  pattern = unnest_reg) %>%
  filter(!word %in% stop_words$word, str_detect(word, "[a-z]"))

# Cleaning process for TDM creating using tm package
ctrl <- list(removePunctuation= list(preserve_intra_word_dashes = T), tolower= T, stopwords= c(stopwords(kind = "en"), removeNumbers= T))
mhatxt <- sapply(mentalhealthawarenesstweets, 
  function(x) x$getText())
mhatxt <- iconv(mhatxt, "latin1", "ASCII", sub = "")
mhatxt <- gsub("http(s?)([^ ]*)", " ", mhatxt, ignore.case = T)
mhatxt <- gsub("&amp", "and", mhatxt)
mhacorpus <- VCorpus(VectorSource(mhatxt))
mhatdm <- TermDocumentMatrix(x= mhacorpus, control= ctrl)
mhadtm <- DocumentTermMatrix(x= mhacorpus, control= ctrl)
mhafreq <- sort(rowSums(as.matrix(mhatdm)), decreasing = T)
mhafreq.df <- data.frame(word = names(mhafreq), 
  freq = mhafreq, stringsAsFactors = F, row.names = NULL)
stxt <- sapply(suicidetweets, 
  function(x) x$getText())
stxt <- iconv(stxt, "latin1", "ASCII", sub = "")
stxt <- gsub("http(s?)([^ ]*)", " ", stxt, ignore.case = T)
stxt <- gsub("&amp", "and", stxt)
scorpus <- VCorpus(VectorSource(stxt))
stdm <- TermDocumentMatrix(x= scorpus, control= ctrl)
sdtm <- DocumentTermMatrix(x= scorpus, control= ctrl)
sfreq <- sort(rowSums(as.matrix(stdm)), decreasing = T)
sfreq.df <- data.frame(word = names(sfreq), 
  freq = sfreq, stringsAsFactors = F, row.names = NULL)
atxt <- sapply(anxietytweets, 
  function(x) x$getText())
atxt <- iconv(atxt, "latin1", "ASCII", sub = "")
atxt <- gsub("http(s?)([^ ]*)", " ", atxt, ignore.case = T)
atxt <- gsub("&amp", "and", atxt)
acorpus <- VCorpus(VectorSource(atxt))
atdm <- TermDocumentMatrix(x= acorpus, control= ctrl)
adtm <- DocumentTermMatrix(x= acorpus, control= ctrl)
afreq <- sort(rowSums(as.matrix(atdm)), decreasing = T)
afreq.df <- data.frame(word = names(afreq), 
  freq = afreq, stringsAsFactors = F, row.names = NULL)
dtxt <- sapply(depressiontweets, 
  function(x) x$getText())
dtxt <- iconv(dtxt, "latin1", "ASCII", sub = "")
dtxt <- gsub("http(s?)([^ ]*)", " ", dtxt, ignore.case = T)
dtxt <- gsub("&amp", "and", dtxt)
dcorpus <- VCorpus(VectorSource(dtxt))
dtdm <- TermDocumentMatrix(x= dcorpus, control= ctrl)
ddtm <- DocumentTermMatrix(x= dcorpus, control= ctrl)
dfreq <- sort(rowSums(as.matrix(dtdm)), decreasing = T)
dfreq.df <- data.frame(word = names(dfreq), 
  freq = dfreq, stringsAsFactors = F, row.names = NULL)
ptxt <- sapply(ptsdtweets, 
  function(x) x$getText())
ptxt <- iconv(ptxt, "latin1", "ASCII", sub = "")
ptxt <- gsub("http(s?)([^ ]*)", " ", ptxt, ignore.case = T)
ptxt <- gsub("&amp", "and", ptxt)
pcorpus <- VCorpus(VectorSource(ptxt))
ptdm <- TermDocumentMatrix(x= pcorpus, control= ctrl)
pdtm <- DocumentTermMatrix(x= pcorpus, control= ctrl)
pfreq <- sort(rowSums(as.matrix(ptdm)), decreasing = T)
pfreq.df <- data.frame(word = names(pfreq), 
  freq = pfreq, stringsAsFactors = F, row.names = NULL)
tottxt <- totaltweet$text
tottxt <- iconv(tottxt, "latin1", "ASCII", sub = "")
tottxt <- gsub("http(s?)([^ ]*)", " ", tottxt, ignore.case = T)
tottxt <- gsub("&amp", "and", tottxt)
totcorpus <- VCorpus(VectorSource(tottxt))
totcorpus <- tm_map(totcorpus, PlainTextDocument)
tottdm <- TermDocumentMatrix(x= totcorpus, control= ctrl)
totdtm <- DocumentTermMatrix(x= totcorpus, control= ctrl)
totfreq <- sort(rowSums(as.matrix(tottdm)), decreasing = T)
totfreq.df <- data.frame(word = names(totfreq), 
  freq = totfreq, stringsAsFactors = F, row.names = NULL)
```
Geocode for locations:
```{r}
register_google(key = google.key)
mhtweets.df <- twListToDF(mentalhealthawarenesstweets)
userinfo <- lookupUsers(mhtweets.df$screenName)
userframe <- twListToDF(userinfo)
userframe.dt <- data.table(userframe)
userframe.dt$location <-
  userframe.dt$location[!userframe.dt$location %in% ""]
userlocation <- geocode(userframe.dt$location[1:2000])
userlocation1 <- geocode(userframe.dt$location[2001:4000])
userlocation2 <- geocode(userframe.dt$location[4001:6000])
userlocation3 <- geocode(userframe.dt$location[6001:8000])
userlocation4 <- geocode(userframe.dt$location[8001:10000])
userlocation5 <- geocode(userframe.dt$location[10001:12000])
userlocation6 <- geocode(userframe.dt$location[12001:14000])
userlocation7 <- geocode(userframe.dt$location[14001:16000])
userlocation8 <- geocode(userframe.dt$location[16001:17066])
location <- bind_rows(userlocation,userlocation1)
```
Exploratory Data Analysis:
```{r,error=FALSE,warning=FALSE,message=FALSE}
wordcloud2(mhafreq.df)
wordcloud2(afreq.df)
wordcloud2(pfreq.df)
wordcloud2(sfreq.df)
wordcloud2(dfreq.df)
wordcloud2(totfreq.df)
wordcloud(mhafreq.df$word,mhafreq.df$freq,max.words=100,colors=brewer.pal(8,"Dark2"),scale=c(3,0.5),random.order=F)
ggplot(totaltweet) + geom_bar(aes(x = isRetweet, fill = isRetweet),
  stat = 'count') + geom_text(stat = 'count', 
  aes(x = isRetweet,label=..count..), vjust = 3) + 
  labs(x = "Retweeted Tweets", y = "Count", 
  fill = "Retweeted", 
  title = "Retweeted and Non-Retweeted\nTweets for All Hashtags")
ggplot(totaltweet) + geom_bar(aes(x = isRetweet, fill = hashtag)) + 
  facet_wrap(~hashtag) + geom_text(stat = 'count',
  aes(x = isRetweet, label=..count..), vjust = 1) +
  labs(x = "Retweeted Tweets", y = "Count", fill = "Hashtag",
  title = "Retweeted and Non-Retweeted\nTweets per Hashtag")
worldMap <- map_data("world")
loc_plot <- ggplot(worldMap)
loc_plot + geom_path(aes(x = long, y = lat, group = group)) +
  geom_point(data = location,aes(x=lon,y=lat),color="RED",
  size=.25) + labs(x = "Longitude", y = "Latitude", 
  title = "Map of Location for #mentalhealthawarness Tweets")
totaltweet %>%
  ungroup() %>% filter(!str_detect(text, "^RT")) %>%
  mutate(text = str_replace_all(text, replace_reg, "")) %>%
  unnest_tokens(word, text, token = "regex", 
  pattern = unnest_reg) %>%
  filter(!word %in% stop_words$word, 
  str_detect(word, "[a-z]")) %>%
  dplyr::count(word) %>%
  with(wordcloud(word, n, max.words = 100,
  colors=brewer.pal(8,"Dark2"), 
  scale=c(1.5,0.3), random.order=F))
hashtag_words <- totaltweet %>% 
  filter(!str_detect(text, "^RT")) %>%
  mutate(text = str_replace_all(text, replace_reg, "")) %>%
  unnest_tokens(word, text, token = "regex", 
  pattern = unnest_reg) %>%
  filter(!word %in% stop_words$word,
  str_detect(word, "[a-z]")) %>% 
  dplyr::count(hashtag, word, sort = TRUE) %>% ungroup()
total_words <- hashtag_words %>% group_by(hashtag) %>%
  dplyr::summarize(total = sum(n))
hashtag_words <- left_join(hashtag_words,total_words)
ggplot(hashtag_words, aes(n/total, fill = hashtag)) +
  geom_histogram(show.legend = FALSE) +
  xlim(NA, 0.0009) +
  facet_wrap(~hashtag, ncol = 2, scales = "free_y") +
  labs(x = "Percentage of Total Word Usage", y = "Frequency", 
  title = "Word Frequency Distribution per Hashtag")
hashtag_tfidf <- hashtag_words %>% bind_tf_idf(word, hashtag, n)
hashtag_tfidf <- hashtag_tfidf %>% dplyr::select(-total) %>%
  arrange(desc(tf_idf))
hashtag_tfidf %>%
  arrange(desc(tf_idf)) %>%
  mutate(word = factor(word, levels = rev(unique(word)))) %>% 
  group_by(hashtag) %>% 
  top_n(15) %>% 
  ungroup %>%
  ggplot(aes(word, tf_idf, fill = hashtag)) +
  geom_col(show.legend = FALSE) +
  labs(x = NULL, y = "tf-idf") +
  facet_wrap(~hashtag, ncol = 2, scales = "free") +
  coord_flip() + labs(y = "Frequency of Use", x = "Word",
  title = "Most Used Words by Hashtag")
```
Sentiment Analysis:
```{r,error=FALSE,message=FALSE,warning=FALSE}
# Sentiment analysis code adapted from:
# https://www.tidytextmining.com/
tweet_sentiment <- tidytweet %>%
  inner_join(get_sentiments("bing")) %>%
  dplyr::count(hashtag, index = linenumber %/% 50,sentiment) %>%
  spread(sentiment, n, fill = 0) %>%
  mutate(sentiment = positive - negative)
ggplot(tweet_sentiment,aes(index, sentiment,fill = hashtag)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~hashtag, ncol = 2, scales = "free_x")
bing_word_counts <- tidytweet %>%
  inner_join(get_sentiments("bing")) %>%
  dplyr::count(word, sentiment, sort = TRUE) %>%
  ungroup()
bing_word_counts %>%
  group_by(sentiment) %>%
  top_n(15) %>%
  ungroup() %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n, fill = sentiment)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~sentiment, scales = "free_y") +
  labs(y = "Contribution to sentiment",x = NULL) +
  coord_flip()
tidytweet %>%
  inner_join(get_sentiments("bing")) %>%
  dplyr::count(word, sentiment, sort = TRUE) %>%
  acast(word ~ sentiment, value.var = "n", fill = 0) %>%
  comparison.cloud(colors = c("#F8766D", "#00BFC4"),
  max.words = 100,scale=c(3,.15))
```



Topic Modeling:
```{r}
ui = unique(totdtm$i)
totdtm_new = totdtm[ui,]
tweet_LDA <- LDA(totdtm_new, k=5, control = list(seed = 1234))
tweet_topics <- tidy(tweet_LDA, matrix = "beta")
tweet_top_terms <- tweet_topics %>% group_by(topic) %>% 
  top_n(15, beta) %>% ungroup() %>% arrange(topic, -beta)
tweet_top_terms %>% mutate(term = reorder(term, beta)) %>%
  ggplot(aes(term, beta, fill=factor(topic))) + 
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  coord_flip()
```



N-Grams:
```{r}
# Code for n-grams was adapted from:
# https://www.tidytextmining.com/
totaltweet$text <- iconv(totaltweet$text, "latin1", "ASCII", 
  sub = "")
totaltweet$text <- gsub("http(s?)([^ ]*)", " ",
  totaltweet$text, ignore.case = T)
totaltweet$text <- gsub("&amp", "and", totaltweet$text)
tweet_bigrams <- totaltweet %>% unnest_tokens(bigram, text,
  token = "ngrams", n = 2)
bigrams_separated <- tweet_bigrams %>% separate(bigram,
  c("word1","word2"), sep = " ")
bigrams_filtered <- bigrams_separated %>% 
  filter(!word1 %in% stop_words$word) %>%
  filter(!word2 %in% stop_words$word)
bigram_counts <- bigrams_filtered %>% 
  dplyr::count(word1, word2, sort = TRUE)
bigrams_united <- bigrams_filtered %>% unite(bigram, word1,
  word2, sep = " ")
bigrams_united_freq <- bigrams_united %>% dplyr::count(bigram, sort = TRUE)
tweet_trigrams <- totaltweet %>% unnest_tokens(trigram, text,
  token = "ngrams", n = 3)
trigrams_separated <- tweet_trigrams %>% separate(trigram,
  c("word1","word2","word3"), sep = " ")
trigrams_filtered <- trigrams_separated %>% 
  filter(!word1 %in% stop_words$word) %>%
  filter(!word2 %in% stop_words$word) %>%
  filter(!word3 %in% stop_words$word)
trigram_counts <- trigrams_filtered %>% 
  dplyr::count(word1, word2, word3, sort = TRUE)
trigrams_united <- trigrams_filtered %>% unite(trigram, word1,
  word2, word3, sep = " ")
trigrams_united_freq <- trigrams_united %>% dplyr::count(trigram,
  sort = TRUE)
tweet_4grams <- totaltweet %>% unnest_tokens(fourgram, text,
  token = "ngrams", n = 4)
fourgrams_separated <- tweet_4grams %>% separate(fourgram,
  c("word1","word2","word3","word4"), sep = " ")
fourgrams_filtered <- fourgrams_separated %>% 
  filter(!word1 %in% stop_words$word) %>%
  filter(!word2 %in% stop_words$word) %>%
  filter(!word3 %in% stop_words$word) %>%
  filter(!word4 %in% stop_words$word)
fourgram_counts <- fourgrams_filtered %>% 
  dplyr::count(word1, word2, word3, word4, sort = TRUE)
fourgrams_united <- fourgrams_filtered %>% unite(fourgram, word1,
  word2, word3, word4, sep = " ")
fourgrams_united_freq <- fourgrams_united %>%
  dplyr::count(fourgram, sort = TRUE)
bigram_tf_idf <- bigrams_united %>% 
  dplyr::count(hashtag, bigram) %>%
  bind_tf_idf(bigram, hashtag, n) %>%
  arrange(desc(tf_idf))
bigram_tf_idf %>%
  arrange(desc(tf_idf)) %>%
  group_by(hashtag) %>%
  top_n(12, tf_idf) %>%
  ungroup() %>%
  dplyr::mutate(bigram = reorder(bigram, tf_idf)) %>%
  ggplot(aes(bigram, tf_idf, fill = hashtag)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ hashtag, ncol = 2, scales = "free") +
  coord_flip() +
  labs(y = "tf-idf of bigram to hashtag",
  x = "")
trigram_tf_idf <- trigrams_united %>% 
  dplyr::count(hashtag, trigram) %>%
  bind_tf_idf(trigram, hashtag, n) %>%
  arrange(desc(tf_idf))
trigram_tf_idf %>%
  arrange(desc(tf_idf)) %>%
  group_by(hashtag) %>%
  top_n(12, tf_idf) %>%
  ungroup() %>%
  dplyr::mutate(trigram = reorder(trigram, tf_idf)) %>%
  ggplot(aes(trigram, tf_idf, fill = hashtag)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ hashtag, ncol = 2, scales = "free") +
  coord_flip() +
  labs(y = "tf-idf of trigram to hashtag",
  x = "")
fourgram_tf_idf <- fourgrams_united %>% 
  dplyr::count(hashtag, fourgram) %>%
  bind_tf_idf(fourgram, hashtag, n) %>%
  arrange(desc(tf_idf))
fourgram_tf_idf %>%
  arrange(desc(tf_idf)) %>%
  group_by(hashtag) %>%
  top_n(12, tf_idf) %>%
  ungroup() %>%
  dplyr::mutate(fourgram = reorder(fourgram, tf_idf)) %>%
  ggplot(aes(fourgram, tf_idf, fill = hashtag)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ hashtag, ncol = 2, scales = "free") +
  coord_flip() +
  labs(y = "tf-idf of 4-gram to hashtag",
  x = "")
```
LSA:
```{r}
tottdm <- removeSparseTerms(tottdm, sparse = 0.99)
tottdm_tfidf <- weightTfIdf(tottdm)
totlsa <- lsa(tottdm_tfidf)
```
Word Associations
```{r}

```



Clustering:
```{r}

```



Classification:
```{r}

```
